# AI Foosball Table

## Michael Rencheck

## Dependencies
The following are other software packages required to run the software for the table:
  - ROS Melodic
  - Pololu tic Software
  - OpenCV

## Installation

Begin by installing some of the external software dependancies:
  - The instructions for installing ROS Melodic are found [here](http://wiki.ros.org/melodic/Installation/Ubuntu).

  - The instructions to install the Pololu tic Software are found [here](https://github.com/pololu/pololu-tic-software). See relevant section in the BUILDING.md file to build the library from source, these were the [directions](https://github.com/pololu/pololu-tic-software/blob/master/BUILDING.md#building-from-source-on-linux-for-linux) I used.

After installing these create a new catkin workspace and clone this repository:

```
mkdir SinglePlayerFoosball
cd SinglePlayerFoosball
git clone https://github.com/rencheckyoself/single_player_foosball.git src
```

Then go into the repository and use wstool. This will download my forked version of the oCam package. It has been updated to function properly with ROS Melodic and adds some extra configuration for including the main launch file.
```
cd src
wstool update
```

Finally, go back to the top level of the directory and build the workspace:
```
cd ..
catkin_make
```

## Required Configuration

- Update `table_motor_control/config/motor_ids.yaml` with the serial numbers for your specific TIC boards. These can be obtained by using the ticgui or ticcli.

- Calibrate your camera. I have included the calibration file for my camera in the package files, but it may not be suitable for yours.

## Packages

### Control
This package provides an interface to work with the TIC Stepper Controller API through ROS by offering services.

Launch Files:
- `tic_controllers.launch`: launch all of the nodes to interact with the motors

Nodes:
 - `tic_cmd`: main node that starts up the services to interact with each controller.

Configuration:
- `motor_ids.yaml`: contains all of the identifying information for each controller. This file must be updated with the serial numbers for your purchased boards.

- `*_settings.txt`: these files contain the detailed settings for each of the motors. They were initially generated by exporting the settings using the ticgui. These files can be used to update the settings of a given tic while everything is running using the `*_update_settings` service offered by the `tic_cmd` node.

### Vision
This package is currently set up to follow the [ROS image_pipeline](http://wiki.ros.org/image_pipeline) structure. The `table_vision_sensing` package is set up to be camera agnostic, so feel free to use your own camera in place of the one linked in the parts list. You will need to modify the launch files to use your own camera.

Launch Files:
- `start_ocam.launch`: Launch the camera and view the rectified image
- `start_tracking.launch`: Use the trained cascade classifier to track the ball and view it
- `capture_training_data.launch`: Launch everything to collect images to train the cascade classifier

Nodes:
 - `cascade_classifier`: Use the trained cascade model to find and publish the ball's pixel location
 - `opencv_viewer`: Listen to an image topic and display the image along with the ball position
 - `training_image_capture`: Listen to an image topic to collect data to train a cascade classifier

#### Training the cascade classifier:
I have included the model I trained in the `cascade_data` folder, but if you don't have the same table/ball as my set up you may need to train your own cascade model.

OpenCV provides a great [walk-through](https://docs.opencv.org/3.4/dc/d88/tutorial_traincascade.html) for how to use their provided tools to generate the final output. Read through this first, then use the following to help with collecting your image dataset and generating the trained model.

To collect your own image set using the camera mounted over the table use the `capture_training_image.launch` file. Start nodes to create a new directory inside the `table_vision_sensing` package called `training_data`. Inside will be the `.dat` files required for the OpenCV training pipeline and two directories corresponding to the saved images. After launching, two windows will be displayed one called "Capture for background" and "Capture for ball". To save an image for the background/negative set, click on the "Capture for background" window and click the "s" key. Follow the same process to save an image for the ball/positive set. All of the images in the background set should not contain the ball.

If you want to add you your collected set later, just relaunch the file and the numbering will pick up where each set left off.

After all of your images are collected, use the integrated annotation tool provided by OpenCV to properly annotate the ball/positive image set. Using the following:
```
opencv_annotation --annotations=[path to table_vision_sensing]/training_data/ball.dat --images=[path to table_vision_sensing]/training_data/ball
```

After the annonations have been completed, use the provided OpenCV tool to create the positive image set .vec file. Use the following command:
```
opencv_createsamples -vec [path to table_vision_sensing]/training_data/ball.vec -info [path to table_vision_sensing]/training_data/ball.dat -w 50 -h 50
```

After the .vec file has been generated, use the cascade trainer to get the trained cascade model for ball detection. The following was my configuration but your mileage may vary.
```
mkdir [path to table_vision_sensing]/cascade_data
opencv_traincascade -data [path to table_vision_sensing]/cascade_data -vec [path to table_vision_sensing]/training_data/ball.vec
```

### Gamplay
This package is used as the main decision making pipeline based on the detected ball position and creates a joint state message that can be used by the urdf model for testing or converted to stepper commands using the `table_motor_control` package.

Launch Files:
- `start_game.launch`: Launches everything to play a game of foosball against the table
- `start_simulation.launch`: Uses the real ball position on the table, but does not actuate the real motors only the urdf model in rviz.

Nodes:
 - `follow_ball`: The main node to decide the action and publish the joint state message.
